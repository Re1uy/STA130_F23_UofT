{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98d56c45",
   "metadata": {},
   "source": [
    "# ~Simple/Multiple Linear Regression~ Binary Classification<br>with Decision Trees\n",
    "\n",
    "- Remember ***extrapolation predictions*** $\\tilde y_i = \\hat \\beta_0 + \\hat \\beta_1 \\tilde x_i$...?<br><br>\n",
    "\n",
    "    - Train-Test Framework: \"In Sample\" versus \"Out of Sample\" performance\n",
    "      \n",
    "      ```python \n",
    "np.random.seed(130)\n",
    "train_index = ab_noNaN.sample(frac=0.80).index.sort_values()\n",
    "isin_train_index = ab_noNaN.index.isin(train_index)\n",
    "test_index = ab_noNaN.index[~isin_train_index]\n",
    "```\n",
    "\n",
    "    - Model Complexity and the Train-Test Framework\n",
    "    \n",
    "        - Multiple Linear Regression Model Building with `statsmodels`\n",
    "\n",
    "          ```python \n",
    "        Q(\"x 0\"), C(x1), I(x2**2), C(x3) * x2, + ...\n",
    "        # C(x3) versus just x3? Training C(x3) versus testing C(x3)?\n",
    "        modfit.model.exog # modfit.model.endog # modfit.model.formula`   \n",
    "```\n",
    "\n",
    "- `sklearn`\n",
    "    \n",
    "  ```python \n",
    "# Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "y = df['continuous outcome']\n",
    "X = pd.get_dummies(df[list_of_covariates], drop_first=True)\n",
    "# Categorical variables with `k` levels only need `k-1` columns...\n",
    "mod = LinearRegression(fit_intercept=True)#False if X already has a 1's column \n",
    "mod.fit(X, y) # don't need to assign to a `modfit` object like `statsmodels`\n",
    "mod.intercept_, mod.coef_, mod.predict()\n",
    "```\n",
    "  ```python \n",
    "# Classification\n",
    "from sklearn import tree\n",
    "y = pd.get_dummies(df['binary outcome'], drop_first=True) \n",
    "# Classification generalizes to categorical outcomes... \n",
    "X = df[list_of_features].astype(float) # might need `pd.get_dummies`\n",
    "tree_depth # Model complexity control\n",
    "clf = tree.DecisionTreeClassifier(max_depth=tree_depth)\n",
    "clf.fit(X,y)\n",
    "```\n",
    "\n",
    "    - Decision Tree visualization with `graphviz`\n",
    "        - Decision Tree already use high order \"interactions\"...\n",
    "        - Decision Trees therefore definitely need \"complexity control\"...\n",
    "        \n",
    "    - Confusion Matrices, Sensitivity, Specificity, Accuracy with \n",
    "        - `from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay`\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6624026f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>List Price</th>\n",
       "      <th>Amazon Price</th>\n",
       "      <th>Hard_or_Paper</th>\n",
       "      <th>NumPages</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Pub year</th>\n",
       "      <th>ISBN-10</th>\n",
       "      <th>Thick</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1,001 Facts that Will Scare the S#*t Out of Yo...</td>\n",
       "      <td>Cary McNeal</td>\n",
       "      <td>12.95</td>\n",
       "      <td>5.18</td>\n",
       "      <td>P</td>\n",
       "      <td>304</td>\n",
       "      <td>Adams Media</td>\n",
       "      <td>2010</td>\n",
       "      <td>1605506249</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21: Bringing Down the House - Movie Tie-In: Th...</td>\n",
       "      <td>Ben Mezrich</td>\n",
       "      <td>15.00</td>\n",
       "      <td>10.20</td>\n",
       "      <td>P</td>\n",
       "      <td>273</td>\n",
       "      <td>Free Press</td>\n",
       "      <td>2008</td>\n",
       "      <td>1416564195</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100 Best-Loved Poems (Dover Thrift Editions)</td>\n",
       "      <td>Smith</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.50</td>\n",
       "      <td>P</td>\n",
       "      <td>96</td>\n",
       "      <td>Dover Publications</td>\n",
       "      <td>1995</td>\n",
       "      <td>486285537</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1421: The Year China Discovered America</td>\n",
       "      <td>Gavin Menzies</td>\n",
       "      <td>15.99</td>\n",
       "      <td>10.87</td>\n",
       "      <td>P</td>\n",
       "      <td>672</td>\n",
       "      <td>Harper Perennial</td>\n",
       "      <td>2008</td>\n",
       "      <td>61564893</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1493: Uncovering the New World Columbus Created</td>\n",
       "      <td>Charles C. Mann</td>\n",
       "      <td>30.50</td>\n",
       "      <td>16.77</td>\n",
       "      <td>P</td>\n",
       "      <td>720</td>\n",
       "      <td>Knopf</td>\n",
       "      <td>2011</td>\n",
       "      <td>307265722</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>Where the Sidewalk Ends</td>\n",
       "      <td>Shel Silverstein</td>\n",
       "      <td>18.99</td>\n",
       "      <td>12.24</td>\n",
       "      <td>H</td>\n",
       "      <td>192</td>\n",
       "      <td>HarperCollins</td>\n",
       "      <td>2004</td>\n",
       "      <td>60572345</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>White Privilege</td>\n",
       "      <td>Paula S. Rothenberg</td>\n",
       "      <td>27.55</td>\n",
       "      <td>27.55</td>\n",
       "      <td>P</td>\n",
       "      <td>160</td>\n",
       "      <td>Worth Publishers</td>\n",
       "      <td>2011</td>\n",
       "      <td>1429233443</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>Why I wore lipstick</td>\n",
       "      <td>Geralyn Lucas</td>\n",
       "      <td>12.95</td>\n",
       "      <td>5.18</td>\n",
       "      <td>P</td>\n",
       "      <td>224</td>\n",
       "      <td>St Martin's Griffin</td>\n",
       "      <td>2005</td>\n",
       "      <td>031233446X</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>Worlds Together, Worlds Apart: A History of th...</td>\n",
       "      <td>Robert Tignor</td>\n",
       "      <td>97.50</td>\n",
       "      <td>97.50</td>\n",
       "      <td>P</td>\n",
       "      <td>480</td>\n",
       "      <td>W. W. Norton &amp; Company</td>\n",
       "      <td>2010</td>\n",
       "      <td>393934942</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>Wuthering Heights</td>\n",
       "      <td>Emily Bronte</td>\n",
       "      <td>16.99</td>\n",
       "      <td>4.95</td>\n",
       "      <td>P</td>\n",
       "      <td>344</td>\n",
       "      <td>CreatSpace</td>\n",
       "      <td>2011</td>\n",
       "      <td>1463533411</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>319 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title               Author  \\\n",
       "0    1,001 Facts that Will Scare the S#*t Out of Yo...          Cary McNeal   \n",
       "1    21: Bringing Down the House - Movie Tie-In: Th...          Ben Mezrich   \n",
       "2         100 Best-Loved Poems (Dover Thrift Editions)                Smith   \n",
       "3              1421: The Year China Discovered America        Gavin Menzies   \n",
       "4      1493: Uncovering the New World Columbus Created      Charles C. Mann   \n",
       "..                                                 ...                  ...   \n",
       "320                            Where the Sidewalk Ends     Shel Silverstein   \n",
       "321                                    White Privilege  Paula S. Rothenberg   \n",
       "322                                Why I wore lipstick        Geralyn Lucas   \n",
       "323  Worlds Together, Worlds Apart: A History of th...        Robert Tignor   \n",
       "324                                  Wuthering Heights         Emily Bronte   \n",
       "\n",
       "     List Price  Amazon Price Hard_or_Paper  NumPages               Publisher  \\\n",
       "0         12.95          5.18             P       304             Adams Media   \n",
       "1         15.00         10.20             P       273              Free Press   \n",
       "2          1.50          1.50             P        96      Dover Publications   \n",
       "3         15.99         10.87             P       672        Harper Perennial   \n",
       "4         30.50         16.77             P       720                   Knopf   \n",
       "..          ...           ...           ...       ...                     ...   \n",
       "320       18.99         12.24             H       192           HarperCollins   \n",
       "321       27.55         27.55             P       160        Worth Publishers   \n",
       "322       12.95          5.18             P       224     St Martin's Griffin   \n",
       "323       97.50         97.50             P       480  W. W. Norton & Company   \n",
       "324       16.99          4.95             P       344              CreatSpace   \n",
       "\n",
       "     Pub year     ISBN-10  Thick  \n",
       "0        2010  1605506249    0.8  \n",
       "1        2008  1416564195    0.7  \n",
       "2        1995   486285537    0.3  \n",
       "3        2008    61564893    1.6  \n",
       "4        2011   307265722    1.4  \n",
       "..        ...         ...    ...  \n",
       "320      2004    60572345    1.1  \n",
       "321      2011  1429233443    0.7  \n",
       "322      2005  031233446X    0.7  \n",
       "323      2010   393934942    0.9  \n",
       "324      2011  1463533411    1.0  \n",
       "\n",
       "[319 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ab = pd.read_csv(\"../../amazonbooks.csv\", encoding=\"ISO-8859-1\")#.dropna()\n",
    "#print(ab.shape)\n",
    "#ab.isnull().sum()\n",
    "ab_noNaN = ab.drop(['Weight_oz','Width','Height'], axis=1).dropna()\n",
    "ab_noNaN['Pub year'] = ab_noNaN['Pub year'].astype(int)\n",
    "ab_noNaN['NumPages'] = ab_noNaN['NumPages'].astype(int)\n",
    "ab_noNaN['Hard_or_Paper'] = ab_noNaN['Hard_or_Paper'].astype(\"category\")\n",
    "#print(ab_noNaN.shape)\n",
    "#ab_noNaN.dtypes\n",
    "ab_noNaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ff9d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(130)\n",
    "#train_index\n",
    "#test_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "161fc7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "923050bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>    <td>Q(\"Amazon Price\")</td> <th>  R-squared:         </th> <td>   0.913</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>        <th>  Adj. R-squared:    </th> <td>   0.912</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>   <th>  F-statistic:       </th> <td>   3307.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 26 Nov 2023</td>  <th>  Prob (F-statistic):</th> <td>9.09e-170</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:38:19</td>      <th>  Log-Likelihood:    </th> <td> -867.62</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   319</td>       <th>  AIC:               </th> <td>   1739.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   317</td>       <th>  BIC:               </th> <td>   1747.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>       <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>     <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>       <td>   -2.6676</td> <td>    0.341</td> <td>   -7.825</td> <td> 0.000</td> <td>   -3.338</td> <td>   -1.997</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"List Price\")</th> <td>    0.8500</td> <td>    0.015</td> <td>   57.506</td> <td> 0.000</td> <td>    0.821</td> <td>    0.879</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>104.183</td> <th>  Durbin-Watson:     </th> <td>   2.015</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1018.441</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.036</td>  <th>  Prob(JB):          </th> <td>7.05e-222</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>11.505</td>  <th>  Cond. No.          </th> <td>    38.2</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:      Q(\"Amazon Price\")   R-squared:                       0.913\n",
       "Model:                            OLS   Adj. R-squared:                  0.912\n",
       "Method:                 Least Squares   F-statistic:                     3307.\n",
       "Date:                Sun, 26 Nov 2023   Prob (F-statistic):          9.09e-170\n",
       "Time:                        11:38:19   Log-Likelihood:                -867.62\n",
       "No. Observations:                 319   AIC:                             1739.\n",
       "Df Residuals:                     317   BIC:                             1747.\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===================================================================================\n",
       "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------\n",
       "Intercept          -2.6676      0.341     -7.825      0.000      -3.338      -1.997\n",
       "Q(\"List Price\")     0.8500      0.015     57.506      0.000       0.821       0.879\n",
       "==============================================================================\n",
       "Omnibus:                      104.183   Durbin-Watson:                   2.015\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1018.441\n",
       "Skew:                           1.036   Prob(JB):                    7.05e-222\n",
       "Kurtosis:                      11.505   Cond. No.                         38.2\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "simple_linear_regression_model = smf.ols('Q(\"Amazon Price\") ~ Q(\"List Price\")', \n",
    "                                         data=ab_noNaN)\n",
    "model_fit = simple_linear_regression_model.fit() # creates line of best fit\n",
    "model_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44988fdf",
   "metadata": {},
   "source": [
    "# Demonstrate model building\n",
    "# Demonstrate \"out of sample\" performance\n",
    "# Demonstrate model complexity... *increasing model complexity*<br>*eventually degrades \"out of sample\" performance*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cf1dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05ab6270",
   "metadata": {},
   "source": [
    "# Transition to `sklearn`\n",
    "\n",
    "\\begin{align*}\n",
    "E[y] ={}& \\beta_0 + \\beta_1 \\times \\text{ListPrice} + \\beta_2 \\times 1_{[\\text{'P'}]}(\\text{Hard_or_Paper})\\; + \\\\{}& \\quad\\;\\;\\; \\beta_3 \\times \\text{ListPrice}\\times 1_{[\\text{'P'}]}(\\text{Hard_or_Paper}) + \\beta_4 \\times \\text{ListPrice}^2\n",
    "\\end{align*}\n",
    "- not hardback: $\\beta_0 + \\beta_1 \\times \\text{ListPrice} + \\beta_4 \\times \\text{ListPrice}^2$\n",
    "- are hardback: $(\\beta_0+\\beta_2) + (\\beta_1+\\beta_3) \\times \\text{ListPrice} + \\beta_4 \\times \\text{ListPrice}^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d583a7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn: THE ML (machine learning) library \n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# X features (ML) [covariates in stats]; y is the outcome\n",
    "X = MLRM_6_fit.model.exog # X must be fully numeric\n",
    "# or we'll get the error: `ValueError: could not convert string to float`\n",
    "y = MLRM_6_fit.model.endog\n",
    "mod = LinearRegression(fit_intercept=False)\n",
    "mod.fit(X, y)\n",
    "mod.intercept_, mod.coef_ # all sklearn gives is y-hats\n",
    "# sklearn does not give any statistical analysis... because\n",
    "# sklearn is a machine learning library only concerned with prediction...\n",
    "# not statistical analysis..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95d4b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLRM_6_fit.summary()\n",
    "MLRM_6_fit.summary().tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935e73b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(ab_noNaN.loc[test_index,\"Hard_or_Paper\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f4776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(ab_noNaN[\"Hard_or_Paper\"])['H']\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b7edbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_noNaN[['NumPages', 'Thick', 'List Price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367bbdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43a0ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ab_noNaN[['NumPages', 'Thick', 'List Price']]\n",
    "\n",
    "clf = clf.fit(X.loc[train_index,:], y[train_index])\n",
    "# Can't do... `formula = \"Hard_or_Paper ~ Height + Width\"`\n",
    "\n",
    "_ = tree.plot_tree(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056642d6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# _ = tree.plot_tree(clf)\n",
    "import graphviz \n",
    "dot_data = tree.export_graphviz(clf, class_names=['Paperback', 'Hardback'], \n",
    "                                feature_names=X.columns,  \n",
    "                                out_file=None, filled=True, rounded=True,\n",
    "                                special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d56298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A confusion matrix is a way to evaluate classification predictions\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "cm_disp = ConfusionMatrixDisplay(confusion_matrix(y[train_index].values, \n",
    "                                                  clf.predict(X.loc[train_index,:]), \n",
    "                                                  labels=[0,1]),\n",
    "                                 display_labels=['P','H'])\n",
    "_ = cm_disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b02981d",
   "metadata": {},
   "source": [
    "# Confusion Matrices are often presented in terms of FP/FN(TP/TN)\n",
    "\n",
    "|         |Pred P(-) |Pred H(+) |\n",
    "|---------|----------|----------|\n",
    "|True P(-)|TrueNegative(TN)|FalsePositive(FP)|       \n",
    "|True H(+)|FalseNegative(FN)|TruePositive(TP)|\n",
    "\n",
    "- **Sensitivity** [proportion of correct predictions in the actually positive class]:<br>TP/(TP+FN), e.g., up above our sensitivity is ?/? = ?% sensitivity\n",
    "- **Specificity** [proportion of correct predictions in the actually negative class]:<br>TN/(TN+FP), e.g., up above our specificity is ?/? = ?% specificity\n",
    "- **Accuracy** [overall proportion of correct predictions]:<br>(TP+TN)/(TP+TN+FP+FN)[=N], e.g., up above our accurac is ?/? = ?%\n",
    "\n",
    "Note that these should be \"out of sample metrics\" -- this should be performance on test data -- not training used to fit the model (which would over optimistically estimate these metric scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f38d0c",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Confusion_matrix#Table_of_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b134af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_disp = ConfusionMatrixDisplay(confusion_matrix(y[test_index].values, \n",
    "                                                  clf.predict(X.loc[test_index,:]), \n",
    "                                                  labels=[0,1]),\n",
    "                                 display_labels=['P','H'])\n",
    "_ = cm_disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c034fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(max_depth=4)\n",
    "clf\n",
    "\n",
    "X = ab_noNaN[['NumPages', 'Thick', 'List Price']]\n",
    "\n",
    "clf = clf.fit(X.loc[train_index,:], y[train_index])\n",
    "# Can't do... `formula = \"Hard_or_Paper ~ Height + Width\"`\n",
    "\n",
    "dot_data = tree.export_graphviz(clf, class_names=['Paperback', 'Hardback'], \n",
    "                                feature_names=X.columns,  \n",
    "                                out_file=None, filled=True, rounded=True,\n",
    "                                special_characters=True)  \n",
    "graphviz.Source(dot_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e6f76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_disp = ConfusionMatrixDisplay(confusion_matrix(y[train_index].values, \n",
    "                                                  clf.predict(X.loc[train_index,:]), \n",
    "                                                  labels=[0,1]),\n",
    "                                 display_labels=['P','H'])\n",
    "_ = cm_disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec9e9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_disp = ConfusionMatrixDisplay(confusion_matrix(y[test_index].values, \n",
    "                                                  clf.predict(X.loc[test_index,:]), \n",
    "                                                  labels=[0,1]),\n",
    "                                 display_labels=['P','H'])\n",
    "_ = cm_disp.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
